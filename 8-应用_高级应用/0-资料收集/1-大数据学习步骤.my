<h1>大数据学习步骤</h1>

<p>　　上面虽然列出来了很多框架，但是最开始学习的时候没必要全部都学，就算是在工作中，这些框架也不一定会全部用到。</p>

<p>　　下面我就大致列一下，各种框架的一个学习步骤吧：</p>

<p><strong>　　注意：下面列出来的顺序只是个人建议，可以根据个人实际情况来调整顺序</strong></p>

<ol start="0">
<li>
<h4>linux基础和javase基础【包含mysql】</h4>

<ul>
<li>这些是基本功，刚开始也不可能学的很精通，最起码要对linux中的一些基本的命令混个脸熟，后面学习各种框架的时候都会用到，用多了就熟悉了。javase的话建议主要看面向对象，集合，io，多线程，以及jdbc操作即可。</li>
</ul>
</li>
<li>
<h4>zookeeper</h4>

<ul>
<li>zookeeper是很多大数据框架的基础，中文名称是动物园的意思，因为目前的大数据框架的图标很多都是动物的形状，所以zookeeper其实就是可以管理很多大数据框架的。针对这个框架，主要掌握如何搭建单节点和集群，以及掌握如何在zkcli客户端下对zookeeper的节点进行增删改查操作即可。</li>
</ul>
</li>
<li>
<h4>hadoop</h4>

<ul>
<li>目前企业中一般都是用hadoop2.x的版本了，所以就没有必要再去学hadoop1.x版本了，hadoop2.x主要包含三大块
<ul>
<li>hdfs 前期，主要学习hdfs的一些命令即可，上传，下载，删除，移动，查看等命令...</li>
<li>mapreduce 这个需要重点学习下，要理解mr的原理以及代码实现，虽然现在工作中真正写mr的代码次数很少了，但是原理还是要理解的。</li>
<li>yarn 前期了解即可，只需要知道yarn是一个资源调度平台，主要负责给任务分配资源即可，yarn不仅可以给mapreduce任务调度资源，还可以为spark任务调度资源...yarn是一个公共的资源调度平台，所有满足条件的框架都可以使用yarn来进行资源调度。</li>
</ul>
</li>
</ul>
</li>
<li>
<h4>hive</h4>

<ul>
<li>hive是一个数据仓库，所有的数据都是存储在hdfs上的，具体【数据仓库和数据库】的区别大家可以去网上搜索一下，有很多介绍。其实如果对mysql的使用比较熟悉的话，使用hive也就简单很多了，使用hive主要是写hql，hql是hive的sql语言，非常类似于mysql数据库的sql，后续学习hive的时候主要理解一些hive的语法特性即可。其实hive在执行hql，底层在执行的时候还是执行的mapredce程序。</li>
<li>注意：其实hive本身是很强大的，数据仓库的设计在工作中也是很重要的，但是前期学习的时候，主要先学会如何使用就好了。后期可以好好研究一下hive。</li>
</ul>
</li>
<li>
<h4>hbase</h4>

<ul>
<li>hbase是一个nosql 数据库，是一个key-value类型的数据库，底层的数据存储在hdfs上。在学习hbase的时候主要掌握 row-key的设计，以及列簇的设计。要注意一个特点就是，hbase基于rowkey查询效率很快，可以达到秒级查询，但是基于列簇中的列进行查询，特别是组合查询的时候，如果数据量很大的话，查询性能会很差。</li>
</ul>
</li>
<li>
<h4>redis</h4>

<ul>
<li>redis也是一个nosql 数据库和key-value类型的数据库，但是这个数据库是纯基于内存的，也就是redis数据库中的数据都是存储在内存中的，所以它的一个特点就是适用于快速读写的应用场景，读写可以达到10W次/秒，但是不适合存储海量数据，毕竟机器的内存是有限的，当然，redis也支持集群，也可以存储大量数据。在学习redis的时候主要掌握string，list，set，sortedset，hashmap这几种数据类型的区别以及使用，还有pipeline管道，这个在批量入库数据的时候是非常有用的，以及transaction事务功能。</li>
</ul>
</li>
<li>
<h4>flume</h4>

<ul>
<li>flume是一个日志采集工具，这个还是比较常用的，最常见的就是采集应用产生的日志文件中的数据。一般有两个流程，一个是flume采集数据存储到kafka中，为了后面使用storm或者sparkstreaming进行实时处理。另一个流程是flume采集的数据落盘到hdfs上，为了后期使用hadoop或者spark进行离线处理。在学习flume的时候其实主要就是学会看flume官网的文档，学习各种组建的配置参数，因为使用flume就是写各种的配置。</li>
</ul>
</li>
<li>
<h4>kafka</h4>

<ul>
<li>kafka 是一个消息队列，在工作中常用于实时处理的场景中，作为一个中间缓冲层，例如，flume->kafka->storm/sparkstreaming。学习kafka主要掌握topic，partition，replicate等的概念和原理。</li>
</ul>
</li>
<li>
<h4>storm</h4>

<ul>
<li>storm是一个实时计算框架，和hadoop的区别就是，hadoop是对离线的海量数据进行处理，而storm是对实时新增的每一条数据进行处理，是一条一条的处理，可以保证数据处理的时效性。学习storm主要学习topology的编写，storm并行度的调整，以及storm如何整合kafka实时消费数据。</li>
</ul>
</li>
<li>
<h4>spark</h4>

<ul>
<li>spark 现在发展的也很不错，也发展成了一个生态圈，spark里面包含很多技术，spark core，spark steaming，spark mlib，spark graphx。</li>
<li>spark生态圈里面包含的有离线处理spark core，和实时处理spark streaming，在这里需要注意一下，storm和spark streaming ，两个都是实时处理框架，但是主要区别是：storm是真正的一条一条的处理，而spark streaming 是一批一批的处理。</li>
<li>spark中包含很多框架，在刚开始学习的时候主要学习spark core和spark streaming即可。这个一般搞大数据的都会用到。spark mlib和spark graphx 可以等后期工作需要或者有时间了在研究即可。</li>
</ul>
</li>
<li>
<h4>elasticsearch</h4>

<ul>
<li>elasticsearch是一个适合海量数据实时查询的全文搜索引擎，支持分布式集群，其实底层是基于lucene的。在查询的时候支持快速模糊查询，求count，distinct，sum，avg等操作，但是不支持join操作。elasticsearch目前也有一个生态圈，elk(elasticsearch logstash kibana)是一个典型的日志收集，存储，快速查询出图表的一整套解决方案。在学习elasticsearch的时候，前期主要学习如何使用es进行增删改查，es中的index，type，document的概念，以及es中的mapping的设计。</li>
</ul>
</li>
</ol>

<p>　　目前暂且列出来这么多吧，大数据生态圈目前还有很多比较好的技术框架，这个就需要等大家以后工作之后再去扩展了。</p>

<p>　　其实上面列出来的这十几个框架，在学习的时候，要专门挑一两个着重研究一下，最好针对，底层原理，优化，源码等部分有所涉猎，这么的话可以在面试过程中脱颖而出。不要想着把每一个框架都搞精通，目前是不现实的，其实就算是在工作中也不会每一个框架都会用的很深。</p>

<p>　　如果能过对上面的框架都大致会使用，并且对某一两个框架研究的比较深的话，其实想去找一份满意的大数据工作也就水到渠成了。</p>

<p>　　上面说的这么多，是根据博主最近几年的一些经验总结吧，如果大家有什么观点可以在下面留言讨论。</p>

<p> 　 <strong> 最终，大家还是要落于找项目来提升自己，必须要有做项目的经验。现在互联网时代，别跟我说，找不到学习的资源。谷歌百度你懂的... !!!</strong></p>

<p>　　其实，说白了，大家还是要在入门之后，有了一定经验，更多还是要去看官网。这是最重要！包括动手去实践，多敲命令！</p>
