<h1>Win10不需要Cygwin搭建大数据测试环境（4）---Hive</h1>

<h2>系列文章</h2>

<p>1：<a href="http://blog.csdn.net/tornadojava/article/details/56013420">《Win10不需要Cygwin搭建大数据测试环境（1）-Hadoop》</a>&nbsp;<br />
2：<a href="http://blog.csdn.net/tornadojava/article/details/56479651">《Win10不需要Cygwin搭建大数据测试环境（2）-HBase》</a>&nbsp;<br />
3：<a href="http://blog.csdn.net/tornadojava/article/details/56494384">《Win10不需要Cygwin搭建大数据测试环境（3）-Java操作HBase》</a>&nbsp;<br />
4：<a href="http://blog.csdn.net/tornadojava/article/details/56835459">《Win10不需要Cygwin搭建大数据测试环境（4）-Hive》</a></p>

<h2>前言</h2>

<p>这篇文章是系列文章中的第四篇，主要描述了win10系安装Hive。</p>

<h2>准备工作</h2>

<p>1：完成<a href="http://blog.csdn.net/tornadojava/article/details/56013420">第一篇教程</a>，确保Hadoop正常运行。&nbsp;<br />
2：下载Hive，我安装的版本是apache-hive-2.1.1-bin。安装路径是D:\apache-hive-2.1.1-bin。&nbsp;<br />
3：设置环境变量 HIVE_HOME=D:\apache-hive-2.1.1-bin。</p>

<h2>元数据（metastore）</h2>

<pre>
<code>这个是Hive独有的概念。
怎么理解呢？
HIVE的功能是将HQL翻译成MapReduce在Hadoop上执行。
元数据的功能就是将HQL翻译成MapReduce所需要的数据。
元数据默认存储在Derby中，建议都用关系型数据库。我的例子是使用了MySql。    
</code></pre>

<h2>部署过程</h2>

<p>几个关键位置&nbsp;<br />
D:\apache-hive-2.1.1-bin\conf&nbsp;<br />
D:\apache-hive-2.1.1-bin\bin&nbsp;<br />
D:\apache-hive-2.1.1-bin\scripts\metastore\upgrade</p>

<h3>1：hive-site.xml</h3>

<p>这是至关重要的配置文件。D:\apache-hive-2.1.1-bin\conf下面本没有这个文件。&nbsp;<br />
但是有hive-default.xml.template这个文件，复制一个hive-default.xml.template，重命名为hive-site.xml.</p>

<p>修改如下配置，每个配置都需要搜索名称（name），然后改成自己的配置（value）。以下四项是关于元数据存储的数据库的配置。</p>

<pre>
<code>&lt;property&gt;
        &lt;name&gt;javax.jdo.option.ConnectionURL&lt;/name&gt;
        &lt;value&gt;jdbc:mysql://localhost:3306/hive?createDatabaseIfNotExist=true&lt;/value&gt;
        &lt;description&gt;JDBC connect string for a JDBC metastore&lt;/description&gt;
    &lt;/property&gt;

    &lt;property&gt;
      &lt;name&gt;javax.jdo.option.ConnectionDriverName&lt;/name&gt;
      &lt;value&gt;com.mysql.jdbc.Driver&lt;/value&gt;
      &lt;description&gt;Driver class name for a JDBC metastore&lt;/description&gt;
    &lt;/property&gt;

    &lt;property&gt;
      &lt;name&gt;javax.jdo.option.ConnectionUserName&lt;/name&gt;
      &lt;value&gt;root&lt;/value&gt;
      &lt;description&gt;username to use against metastore database&lt;/description&gt;
    &lt;/property&gt;

    &lt;property&gt;
      &lt;name&gt;javax.jdo.option.ConnectionPassword&lt;/name&gt;
      &lt;value&gt;root&lt;/value&gt;
      &lt;description&gt;password to use against metastore database&lt;/description&gt;
    &lt;/property&gt;</code></pre>

<p>接下来就是修改类似于 &ldquo;<code>${system:</code>&rdquo; 的配置了。在hive-site.xml中有很多地方引用了这种形式的变量，但是在实际运行时，在windows环境下这个变量存在问题。因此统一修改成&nbsp;<br />
相对路径 &lsquo;<code>hive_home</code>&lsquo;.使相关的文件尽量保存在同一个目录下。&nbsp;<br />
相关的修改如下：</p>

<pre>
<code> &lt;property&gt;
    &lt;name&gt;hive.exec.local.scratchdir&lt;/name&gt;    
    &lt;value&gt;hive_home/scratch_dir&lt;/value&gt;
    &lt;description&gt;Local scratch space for Hive jobs&lt;/description&gt;
  &lt;/property&gt;

  &lt;property&gt;
    &lt;name&gt;hive.downloaded.resources.dir&lt;/name&gt;    
    &lt;value&gt;hive_home/resources_dir/${hive.session.id}_resources&lt;/value&gt;    
    &lt;description&gt;Temporary local directory for added resources in the remote file system.&lt;/description&gt;
  &lt;/property&gt;

   &lt;property&gt;
    &lt;name&gt;hive.querylog.location&lt;/name&gt;
    &lt;value&gt;hive_home/querylog_dir&lt;/value&gt;
    &lt;description&gt;Location of Hive run time structured log file&lt;/description&gt;
  &lt;/property&gt;

   &lt;property&gt;
    &lt;name&gt;hive.server2.logging.operation.log.location&lt;/name&gt;
    &lt;value&gt;hive_home/operation_dir&lt;/value&gt;
    &lt;description&gt;Top level directory where operation logs are stored if logging functionality is enabled&lt;/description&gt;
  &lt;/property&gt;</code></pre>

<h3>2:hive-log4j2.properties</h3>

<p>将hive-log4j2.properties.template这个文件复制，重命名为hive-log4j2.properties.&nbsp;<br />
将如下内容直接替换。</p>

<pre>
<code>status = INFO
name = HiveLog4j2
packages = org.apache.hadoop.hive.ql.log

# list of properties
property.hive.log.level = INFO
property.hive.root.logger = DRFA
property.hive.log.dir = hive_log
property.hive.log.file = hive.log
property.hive.perflogger.log.level = INFO

# list of all appenders
appenders = console, DRFA

# console appender
appender.console.type = Console
appender.console.name = console
appender.console.target = SYSTEM_ERR
appender.console.layout.type = PatternLayout
appender.console.layout.pattern = %d{ISO8601} %5p [%t] %c{2}: %m%n

# daily rolling file appender
appender.DRFA.type = RollingRandomAccessFile
appender.DRFA.name = DRFA
appender.DRFA.fileName = ${hive.log.dir}/${hive.log.file}
# Use %pid in the filePattern to append &lt;process-id&gt;@&lt;host-name&gt; to the filename if you want separate log files for different CLI session
appender.DRFA.filePattern = ${hive.log.dir}/${hive.log.file}.%d{yyyy-MM-dd}
appender.DRFA.layout.type = PatternLayout
appender.DRFA.layout.pattern = %d{ISO8601} %5p [%t] %c{2}: %m%n
appender.DRFA.policies.type = Policies
appender.DRFA.policies.time.type = TimeBasedTriggeringPolicy
appender.DRFA.policies.time.interval = 1
appender.DRFA.policies.time.modulate = true
appender.DRFA.strategy.type = DefaultRolloverStrategy
appender.DRFA.strategy.max = 30

# list of all loggers
loggers = NIOServerCnxn, ClientCnxnSocketNIO, DataNucleus, Datastore, JPOX, PerfLogger

logger.NIOServerCnxn.name = org.apache.zookeeper.server.NIOServerCnxn
logger.NIOServerCnxn.level = WARN

logger.ClientCnxnSocketNIO.name = org.apache.zookeeper.ClientCnxnSocketNIO
logger.ClientCnxnSocketNIO.level = WARN

logger.DataNucleus.name = DataNucleus
logger.DataNucleus.level = ERROR

logger.Datastore.name = Datastore
logger.Datastore.level = ERROR

logger.JPOX.name = JPOX
logger.JPOX.level = ERROR

logger.PerfLogger.name = org.apache.hadoop.hive.ql.log.PerfLogger
logger.PerfLogger.level = ${hive.perflogger.log.level}

# root logger
rootLogger.level = ${hive.log.level}
rootLogger.appenderRefs = root
rootLogger.appenderRef.root.ref = ${hive.root.logger}</code></pre>

<h3>3：导数据</h3>

<p>前面说了元数据需要数据库的支持，默认在linux下，可以使用如下命令完成数据库的初始化</p>

<blockquote>
<p>schematool -dbType mysql -initSchema</p>
</blockquote>

<p>但是windows下研究了半天好似不行，跟踪了一下执行的命令，最后发现</p>

<blockquote>
<p>D:\apache-hive-2.1.1-bin\scripts\metastore\upgrade</p>
</blockquote>

<p>这个路径下有数据库初始化的SQL脚本，尝试了一下竟然成功了。</p>

<p>将mysql\hive-schema-2.1.0.mysql.sql这个脚本导入到数据库中。用工具也行,用mysql命令也行。</p>

<pre>
<code>mysql命令&gt;SOURCE SOURCE D:\apache-hive-2.1.1-bin\scripts\metastore\upgrade\mysql\hive-txn-schema-2.1.0.mysql.sql;

</code>这一步执行应该不会有错误，如果有错误，看是不是选错文件夹了，我第一次选错了。</pre>

<h3>4：hadoop创建文件夹</h3>

<p>hadoop上需要创建目录/user/hive/warehouse/的文件夹</p>

<h3>5：启动Hadoop</h3>

<h3>6：启动hive</h3>

<p>启动metastore</p>

<pre>
<code>&gt;hive --service metastore -hiveconf hive.root.logger=DEBUG</code>启动hiveserver</pre>

<pre>
<code>&gt;hive --service hiveserver2</code>
</pre>

<p>启动客户端</p>

<pre>
<code>&gt;hive --service cli </code>
</pre>

<p>如果没有报错，表示win10安装hive成功。</p>

<h3>7：执行测试语句</h3>

<pre>
<code>hive&gt;create table test_table(id INT, name string);</code>
</pre>

<pre>
<code>hive&gt;show tables;</code>
</pre>

<p>这个test_table应该可以在Hadoop的路径下看到。</p>

<h3>后续内容</h3>

<p>beeline的使用&nbsp;<br />
同HBase的整合&nbsp;<br />
同Spark的整合</p>

<h3>猜测</h3>

<p>Hive和Hadoop的master是部署在一起的，那是否可以理解为Hive实际上是通过环境变量HADOOP_HOME来读取hadoop的配置，同Hadoop进行交互的。&nbsp;<br />
因为Hive中并没有任何Hadoop的配置信息。</p>
