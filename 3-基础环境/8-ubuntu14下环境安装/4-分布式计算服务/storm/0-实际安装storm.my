<h1>Storm集群的安装配置</h1>

<h2>strom 启停</h2>

<h3>启动前，确认已启动zookeeper。</h3>

<h3>master 上：</h3>

<p><strong>nohup</strong> /home/storm/soft/storm1.2.2/bin/storm<strong> nimbus &gt;&gt; /dev/null &amp;</strong></p>

<p><strong>nohup</strong> /home/storm/soft/storm1.2.2/bin/storm<strong> drpc &gt;&gt; /dev/null &amp;</strong></p>

<p><strong>nohup</strong> /home/storm/soft/storm1.2.2/bin/storm<strong> ui&nbsp;&gt;&gt; /dev/null &amp;</strong></p>

<h3>slave 上：</h3>

<p><strong>nohup</strong> /home/storm/soft/storm1.2.2/bin/storm <strong>supervisor</strong><strong>&nbsp;&gt;&gt; /dev/null &amp;</strong></p>

<h2>安装步骤</h2>

<p>　　Storm集群的安装分为以下几步：</p>

<p>　　1、首先保证Zookeeper集群服务的正常运行以及必要组件的正确安装</p>

<p>　　2、下载安装包，释放压缩包</p>

<p>　　3、修改storm.yaml添加集群配置信息</p>

<p>　　4、使用storm脚本启动相应服务并查看服务状态</p>

<p>　　5、通过web查看storm集群的状态</p>

<h2>安装必要的依赖组件</h2>

<p>接下来你需要在集群中的所有机器上安装必要的依赖组件，包括：</p>

<ol>
	<li>Java 7（推荐使用 JDK 7 以上版本 &mdash;&mdash; 译者注）</li>
	<li>Python 2.7.*（推荐使用 Python 2.7.x 版本 &mdash;&mdash; 译者注）</li>
</ol>

<p>以上均为在 Storm 上测试通过的版本。Storm 并不保证对其他版本的 Java 或 Python 的支持。</p>

<p>保证Zookeeper集群服务的正常运行</p>

<p>　　安装Storm之前首先保证之前安装的Zookeeper服务正常运行，包括配置hosts映射，主机名修改，防火墙都已经设置完好</p>

<p>　　这里测试使用的三台虚拟机主机名分别是：master，slaveA，slaveB （可以到你的 /etc/hosts 文件查看）</p>

<h2>下载安装包，释放压缩包</h2>

<p>接下来就要下载需要的 Storm 发行版，并将 zip 安装文件解压缩到集群中的各个机器上。Storm 的发行版可以在<a href="http://github.com/apache/storm/releases">这里</a>下载（推荐在 Storm 官方网站的<a href="http://storm.apache.org/downloads.html" target="_blank">下载页面</a>使用 Apache 的镜像服务下载 &mdash;&mdash; 译者注）。</p>

<p>本次安装使用这个：wget http://mirrors.hust.edu.cn/apache/storm/apache-storm-1.2.2/apache-storm-1.2.2.tar.gz</p>

<p>这里下载的是storm 1.2.2的版本，下载之后上传到服务器目录下，释放并且放到指定的目录：</p>

<pre>
# tar -xvzf apache-storm-1.2.2.tar.gz
# mv apache-storm-1.2.2 storm1.2.2
# mkdir /home/storm/soft /home/storm/data
# cd /home/strom/soft/storm1.2.2</pre>

<h2>修改配置文件storm.yaml</h2>

<p><a href="https://www.cnblogs.com/freeweb/p/5179410.html">参考</a>&nbsp;&nbsp;接下来需要修改配置文件storm.yaml，执行vim conf/storm.yaml打开文件：</p>

<h3>storm.zookeeper.servers</h3>

<p>去掉storm.zookeeper.servers:前面的注释，修改为集群中所有部署zookeeper的主机，当然都可以自己手动添加，具体配置如下：</p>

<p>storm.zookeeper.servers:<br />
&nbsp; - &quot;master&quot;<br />
&nbsp; - &quot;slaveA&quot;<br />
&nbsp; - &quot;slaveB&quot;</p>

<h3>storm.local.dir</h3>

<p>增加storm.local.dir选项，指定nimbus，supervisor进程用于存储少量的状态数据，比如jar包，配置文件等</p>

<p># ##### These may optionally be filled in:<br />
#<br />
storm.local.dir: &quot;/home/storm/data/storm&quot;</p>

<p>待会写好配置文件我们需要手动建立这个目录</p>

<h3>supervisor.slots.ports</h3>

<p>下面指定supervisor工作节点，需要配置该节点可以运行的worker数量，每个worker占用一个端口用于接收消息，最多分配5个；默认情况下每个节点可以运行4个worker，分别在6700、6701、6702、6703端口，这里定义3个端口，代表最多运行3个worker：</p>

<p>supervisor.slots.ports:<br />
&nbsp; - 6700<br />
&nbsp; - 6701&nbsp;<br />
&nbsp; - 6702</p>

<h3>nimbus.host</h3>

<p>下面设置集群主机，让集群中所有的节点可以从主机下载拓扑以及配置文件，主机上运行的就是nimbus，而其他节点就是supervisor进程，这里hadoopha为nimbus，而hadoop1和hadoop2为supervisor，所以配置如下：</p>

<p>nimbus.host: &quot;master&quot;</p>

<h3>drpc.servers</h3>

<p>下面配置storm集群的drpc地址，这里就是hadoopha，实际中可以自己定义：</p>

<p>drpc.servers:<br />
&nbsp; - &quot;master&quot;</p>

<h3>worker.childopts</h3>

<p>最后配置storm进程的分配内存，默认情况下Storm启动worker进程时，JVM的最大内存是768M，由于在使用过程中，Bolt中加载大量数据，768M内存无法满足要求，会导致内存溢出，应该根据实际情况进行修改，这里设置为2G</p>

<p>worker.childopts: &quot;-Xmx2048m&quot;</p>

<h2>拷贝storm目录到slave上</h2>

<p>上面的配置是在master上配置的，接下来要把storm目录发送到slaveA和slaveB：</p>

<pre>
# scp -r storm root@slaveA:/home
# scp -r storm root@slaveB:/home</pre>

<h2>启动storm服务</h2>

<p>发送之后，进入storm安装目录，开始启动相应服务</p>

<p>首先启动Nimbus服务，只在master上执行：</p>

<p><strong><span style="font-size:14px">nohup bin/storm nimbus &gt;&gt; /dev/null &amp;</span></strong></p>

<p>上面命令的意思是丢弃输出信息并且放到后台执行，稍微等一下，执行jps查看nimbus进程是否启动：</p>

<p>然后在slaveA，slaveB 节点都启动Supervisor服务：</p>

<pre>
<strong>nohup</strong> bin/<strong>storm supervisor</strong> &gt;&gt; /dev/null <strong>&amp;</strong></pre>

<p>稍等一下，也可以用jps查看到supervisor进程，</p>

<p>然后在配置drpc的主机master，drpc是一种后台服务，用于执行和storm相同的计算，但是比较节省资源，一般和nimbus使用同一台主机即可；执行以下命令启动drpc服务：</p>

<pre>
nohup bin/storm drpc &gt;&gt; /dev/null &amp;</pre>

<p>稍等一下可以分别通过jps命令查看到drpc进程，</p>

<p>最后在nimbus节点也就是hadoopha执行以下命令，启动UI服务：</p>

<pre>
nohup bin/storm ui &gt;&gt; /dev/null &amp;</pre>

<p>　　通过jps可以查看core进程是否启动，启动之后通过访问hadoopha的ip即可进入web管理界面：http://192.168.1.42:8080</p>

<p>　　到这里基本的storm集群就配置完毕了</p>
