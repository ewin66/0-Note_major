<h1>实际安装scala</h1>

<h2>预备环境安装</h2>

<p>首先安装java/scala/hadoop，参考运行环境安装和hadoop安装。</p>

<h2>下载spark</h2>

<p>1.首先在官网下载http://spark.apache.org/downloads.html，选择与scala对应的版本，对应关系为：spark1.6.2--scala2.10；spark2.0.0--scala2.11</p>

<p>选择版本后点击下面链接，进入后会有镜像地址。</p>

<p>Download Spark:&nbsp;<a href="https://www.apache.org/dyn/closer.lua/spark/spark-2.3.1/spark-2.3.1-bin-hadoop2.7.tgz" onclick="trackOutboundLink(this, 'Release Download Links', 'apache_spark-2.3.1-bin-hadoop2.7.tgz'); return false;">spark-2.3.1-bin-hadoop2.7.tgz</a></p>

<p>使用wget + 镜像地址，直接下载。</p>

<p>wget http://mirror.bit.edu.cn/apache/spark/spark-2.3.1/spark-2.3.1-bin-hadoop2.7.tgz ls</p>

<h2>解压spark</h2>

<p>输入tar -zxvf&nbsp;<a href="http://d3kbcqa49mib13.cloudfront.net/spark-1.6.2-bin-hadoop2.6.tgz" rel="nofollow" target="_blank">spark-1.6.2-bin-hadoop2.6.tgz</a>进行解压</p>

<p>3.在使用spark-shell时去除WARN级别的日志</p>

<p>1) 首先cd conf/ 进入到conf文件夹</p>

<p>2) 将log4j文件复制出来一份 cp &nbsp;log4j.properties.template&nbsp;log4j.properties</p>

<p>3) 进入文件并将rootCategory 的级别改成WARN，输入 vi&nbsp;log4j.properties</p>

<p>&nbsp;</p>

<p>　以下操作都在Master节点进行。</p>

<p>　　1）下载二进制包spark-2.1.0-bin-hadoop2.7.tgz</p>

<p>　　2）解压并移动到相应目录，命令如下：</p>

<p>　　tar -zxvf&nbsp;spark-2.1.0-bin-hadoop2.7.tgz</p>

<p>　　mv hadoop-2.7.3 /opt</p>

<p>　　3）修改相应的配置文件。</p>

<p>　　修改/etc/profie，增加如下内容：</p>

<pre>
export SPARK_HOME=/home/spark/spark-2.3.1-bin-hadoop2.7/
export PATH=$PATH:$SPARK_HOME/bin
</pre>

<p>　　4）修改spark目录下conf下文件：</p>

<p>　　复制spark-env.sh.template成spark-env.sh</p>

<p>　　cp spark-env.sh.template spark-env.sh</p>

<p>　　修改$SPARK_HOME/conf/spark-env.sh，添加如下内容：</p>

<p>&nbsp; &nbsp; export JAVA_HOME=/all/java/jdk1.8<br />
&nbsp; &nbsp; export SCALA_HOME=/all/scala/scala-2.11.6<br />
&nbsp; &nbsp; export HADOOP_HOME=/home/hadoop/soft/hadoop-2.7.7<br />
&nbsp; &nbsp; export HADOOP_CONF_DIR=/home/hadoop/soft/hadoop-2.7.7/etc/hadoop<br />
&nbsp; &nbsp; export SPARK_MASTER_IP=master<br />
&nbsp; &nbsp; export SPARK_MASTER_HOST=master<br />
&nbsp; &nbsp; export SPARK_LOCAL_IP=master<br />
&nbsp; &nbsp; export SPARK_WORKER_MEMORY=1g<br />
&nbsp; &nbsp; export SPARK_WORKER_CORES=2<br />
&nbsp; &nbsp; export SPARK_HOME=/home/spark/spark-2.3.1-bin-hadoop2.7<br />
&nbsp; &nbsp; export SPARK_DIST_CLASSPATH=$(/home/hadoop/soft/hadoop-2.7.7/bin/hadoop classpath)</p>

<p>　　复制slaves.template成slaves</p>

<p>　　cp slaves.template slaves</p>

<p>　　修改$SPARK_HOME/conf/slaves，添加如下内容：</p>

<pre>
&nbsp;Master
&nbsp;SlaveA
&nbsp;SlaveB</pre>

<p>　　4）将配置好的spark文件复制到SlaveA和SlaveB节点。</p>

<p>　　scp -r /home/spark root@SlaveA:/home/</p>

<p>&nbsp; &nbsp; &nbsp;&nbsp;scp -r /home/spark root@SlaveB:/home/</p>

<p>　　5）修改SlaveA和SlaveB配置。</p>

<p>　　在SlaveA和SlaveB上分别修改/etc/profile，增加Spark的配置，过程同Master一样。</p>

<p>　　在SlaveA和SlaveB修改$SPARK_HOME/conf/spark-env.sh，将export SPARK_LOCAL_IP=master改成SlaveA和SlaveB对应节点的IP或主机名。</p>

<p>　　6）在Master节点启动集群。</p>

<p>　　/opt/spark-2.1.0-bin-hadoop2.7/sbin/start-all.sh</p>

<p>　　7）查看集群是否启动成功：</p>

<p>　　jps</p>

<p>　　Master在Hadoop的基础上新增了：</p>

<p>　　Master</p>

<p>　　Slave在Hadoop的基础上新增了：</p>

<p>　　Worker</p>
