<p>1. 安装jdk。<br />
1.1 在用户主目录下面建立jdk文件夹<br />
[ghostfire@turing]$ mkdir jdk</p>

<p>1.2 解压缩 jdk.tar.gz 文件到jdk文件夹中<br />
[ghostfire@turing]$ tar -zxvf ./jdk.tar.gz ./jdk/</p>

<p>1.3 将java路径加入path中<br />
[ghostfire@turing]$ vim ～/.bashrc<br />
在文件末尾添加如下内容并保存。<br />
&nbsp; &nbsp; export JAVA_HOME=~/jdk/jdk1.7.0_09<br />
&nbsp; &nbsp; export CLASSPATH=.:$JAVA_HOME/lib/*jar<br />
&nbsp; &nbsp; export PATH=$JAVA_HOME/bin:$PATH<br />
执行如下命令，使得我们设置的path能够马上生效。<br />
[ghostfire@turing]$ source ~/.bashrc</p>

<p>1.5 检测jdk是否安装成功<br />
[ghostfire@turing]$ java -version</p>

<p><br />
2. 验证并安装ssh<br />
2.1 首先检查是否安装了ssh<br />
[ghostfire@turing]$ which ssh<br />
[ghostfire@turing]$ which sshd<br />
[ghostfire@turing]$ which ssh-keygen</p>

<p>2.2 如果提示没有安装或者无任何内容显示，执行如下命令安装ssh<br />
[ghostfire@turing]$ sudo apt-get install openssh-client<br />
[ghostfire@turing]$ sudo apt-get install openssh-server</p>

<p>2.3 检测sshd服务是否启动<br />
[ghostfire@turing]$ ps aux | grep sshd<br />
结果中若显示sshd(注意显示 grep sshd不算)，则sshd服务成功启动,否则执行如下命令启动sshd服务<br />
[ghostfire@turing]$ sudo /etc/init.d/ssh start<br />
注意在有些版本下,命令可能是 sudo /etc/init.d/sshd start</p>

<p>3.生成ssh秘钥对<br />
3.1 生成ssh公钥<br />
[ghostfire@turing]$ ssh-keygen -t rsa<br />
待输入的地方全部回车选择默认<br />
执行完毕后，会在 ~/.ssh/下面生成私钥 id_rsa,公钥id_rsa.pub</p>

<p>3.2 公钥添加<br />
[ghostfire@turing]$ cat ~/.ssh/id_rsa.pub &gt;&gt; ~/.ssh/authorized_keys<br />
[ghostfire@turing]$ chmod 600 ~/.ssh/authorized_keys</p>

<p>3.3 检测公钥是否配置完成<br />
[ghostfire@turing]$ ssh localhost<br />
如果配置成功，则不需要密码就可以通过ssh进入localhost</p>

<p>4.安装hadoop<br />
4.1 在用户主目录下建立hadoop文件夹<br />
[ghostfire@turing]$ mkdir hadoop</p>

<p>4.2 解压缩hadoop-1.0.4.tar.gz<br />
[ghostfire@turing]$ tar -zxvf ./hadoop-1.0.4.tar.gz ./hadoop</p>

<p>4.3 将hadoop路径加入path<br />
[ghostfire@turing]$ vim ～/.bashrc<br />
在文件末尾添加如下内容并保存。<br />
&nbsp; &nbsp; export HADOOP_HOME=~/hadoop/hadoop-1.0.4<br />
&nbsp; &nbsp; export PATH=$HADOOP_HOME/bin:$PATH<br />
执行如下命令，使得我们设置的path能够马上生效。<br />
[ghostfire@turing]$ source ~/.bashrc</p>

<p>4.4 配置hadoop-env.sh<br />
修改~/hadoop/hadoop-1.0.4/conf/hadoop-env.sh<br />
在该文件最后一行添加<br />
export JAVA_HOME=~/jdk/jdk1.7.0_09</p>

<p>5. 配置单机模式<br />
对conf目录下面的配置文件不做修改即为单机模式</p>

<p>6. 配置伪分布模式<br />
6.1 修改core-site.xml文件，内容如下<br />
&lt;?xml version=&quot;1.0&quot;?&gt;<br />
&lt;?xml-stylesheet type=&quot;text/xsl&quot; href=&quot;configuration.xsl&quot;?&gt;<br />
&lt;configuration&gt;<br />
&nbsp; &nbsp; &lt;property&gt;<br />
&nbsp; &nbsp; &nbsp; &nbsp; &lt;name&gt;fs.default.name&lt;/name&gt;<br />
&nbsp; &nbsp; &nbsp; &nbsp; &lt;value&gt;hdfs://localhost:9000&lt;/value&gt;<br />
&nbsp; &nbsp; &lt;/property&gt;<br />
&lt;/configuration&gt;</p>

<p>6.2 修改mapred-site.xml文件，内容如下<br />
&lt;?xml version=&quot;1.0&quot;?&gt;<br />
&lt;?xml-stylesheet type=&quot;text/xsl&quot; href=&quot;configuration.xsl&quot;?&gt;<br />
&lt;configuration&gt;<br />
&nbsp; &nbsp; &lt;property&gt;<br />
&nbsp; &nbsp; &nbsp; &nbsp; &lt;name&gt;mapred.job.tracker&lt;/name&gt;<br />
&nbsp; &nbsp; &nbsp; &nbsp; &lt;value&gt;localhost:9001&lt;/value&gt;<br />
&nbsp; &nbsp; &lt;/property&gt;<br />
&lt;/configuration&gt;</p>

<p>6.3 修改hdfs-site.xml文件，内容如下<br />
&lt;?xml version=&quot;1.0&quot;?&gt;<br />
&lt;?xml-stylesheet type=&quot;text/xsl&quot; href=&quot;configuration.xsl&quot;?&gt;<br />
&lt;configuration&gt;<br />
&nbsp; &nbsp; &lt;property&gt;<br />
&nbsp; &nbsp; &nbsp; &nbsp; &lt;name&gt;dfs.replication&lt;/name&gt;<br />
&nbsp; &nbsp; &nbsp; &nbsp; &lt;value&gt;1&lt;/value&gt;<br />
&nbsp; &nbsp; &lt;/property&gt;<br />
&lt;configuration&gt;</p>

<p>6.4 将localhost添加到hadoop conf目录下面的masters文件中<br />
[ghostfire@turing]$ echo &quot;localhost&quot; &gt;&gt; masters</p>

<p>6.5 将localhost添加到hadoop conf目录下面的slaves文件中<br />
[ghostfire@turing]$ echo &quot;localhost&quot; &gt;&gt; slaves</p>

<p>7. 格式化HDFS<br />
[ghostfire@turing]$ ~/hadoop/hadoop-1.0.4/bin/hadoop namenode -format</p>

<p>8. 启动hadoop<br />
[ghostfire@turing]$ ~/hadoop/hadoop-1.0.4/bin/start-all.sh</p>

<p>9. 检测hadoop是否成功启动<br />
[ghostfire@turing]$ jps<br />
TaskTracker<br />
SecondaryNameNode<br />
NameNode<br />
DateNode<br />
JobTracker</p>

<p>10. 在HDFS中添加文件和目录<br />
[ghostfire@turing]$ hadoop fs -mkdir /user/[你的用户名]/wordcount/input<br />
上面的命令本质上是递归创建的,但在有的版本上是不支持的,那么需要你依次执行如下命令<br />
[ghostfire@turing]$ hadoop fs -mkdir /user<br />
[ghostfire@turing]$ hadoop fs -mkdir /user/[你的用户名]<br />
[ghostfire@turing]$ hadoop fs -mkdir /user/[你的用户名]/wordcount<br />
[ghostfire@turing]$ hadoop fs -mkdir /user/[你的用户名]/wordcount/input</p>

<p><br />
在当前目录下面创建若干个文本文件，每个文件里面添加若干个英文单词，比如<br />
[ghostfire@turing]$ cat input1.txt<br />
no zuo no die<br />
you can you up<br />
[ghostfire@turing]$ cat input2.txt<br />
you can you up<br />
no zuo no die</p>

<p>将文本文件从本地目录上传到HDFS中<br />
[ghostfire@turing]$ hadoop fs -put ./input1.txt /user/[你的用户名]/wordcount/input<br />
[ghostfire@turing]$ hadoop fs -put ./input2.txt /user/[你的用户名]/wordcount/input</p>

<p>查看文件上传是否成功<br />
[ghostfire@turing]$ hadoop fs -lsr /</p>

<p>11. 在当前目录下新建一个WordCount.java文件<br />
import java.io.IOException;<br />
import java.util.StringTokenizer;<br />
import org.apache.hadoop.conf.Configuration;<br />
import org.apache.hadoop.fs.Path;<br />
import org.apache.hadoop.io.IntWritable;<br />
import org.apache.hadoop.io.Text;<br />
import org.apache.hadoop.mapreduce.Job;<br />
import org.apache.hadoop.mapreduce.Mapper;<br />
import org.apache.hadoop.mapreduce.Reducer;<br />
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;<br />
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;<br />
import org.apache.hadoop.util.GenericOptionsParser;</p>

<p>public class WordCount {</p>

<p>&nbsp; public static class TokenizerMapper&nbsp;<br />
&nbsp; &nbsp; &nbsp; &nbsp;extends Mapper&lt;Object, Text, Text, IntWritable&gt;{<br />
&nbsp; &nbsp;&nbsp;<br />
&nbsp; &nbsp; private final static IntWritable one = new IntWritable(1);<br />
&nbsp; &nbsp; private Text word = new Text();<br />
&nbsp; &nbsp; &nbsp;&nbsp;<br />
&nbsp; &nbsp; public void map(Object key, Text value, Context context<br />
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; ) throws IOException, InterruptedException {<br />
&nbsp; &nbsp; &nbsp; StringTokenizer itr = new StringTokenizer(value.toString());<br />
&nbsp; &nbsp; &nbsp; while (itr.hasMoreTokens()) {<br />
&nbsp; &nbsp; &nbsp; &nbsp; word.set(itr.nextToken());<br />
&nbsp; &nbsp; &nbsp; &nbsp; context.write(word, one);<br />
&nbsp; &nbsp; &nbsp; }<br />
&nbsp; &nbsp; }<br />
&nbsp; }<br />
&nbsp;&nbsp;<br />
&nbsp; public static class IntSumReducer&nbsp;<br />
&nbsp; &nbsp; &nbsp; &nbsp;extends Reducer&lt;Text,IntWritable,Text,IntWritable&gt; {<br />
&nbsp; &nbsp; private IntWritable result = new IntWritable();</p>

<p>&nbsp; &nbsp; public void reduce(Text key, Iterable&lt;IntWritable&gt; values,&nbsp;<br />
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;Context context<br />
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;) throws IOException, InterruptedException {<br />
&nbsp; &nbsp; &nbsp; int sum = 0;<br />
&nbsp; &nbsp; &nbsp; for (IntWritable val : values) {<br />
&nbsp; &nbsp; &nbsp; &nbsp; sum += val.get();<br />
&nbsp; &nbsp; &nbsp; }<br />
&nbsp; &nbsp; &nbsp; result.set(sum);<br />
&nbsp; &nbsp; &nbsp; context.write(key, result);<br />
&nbsp; &nbsp; }<br />
&nbsp; }</p>

<p>&nbsp; public static void main(String[] args) throws Exception {<br />
&nbsp; &nbsp; Configuration conf = new Configuration();<br />
&nbsp; &nbsp; String[] otherArgs = new GenericOptionsParser(conf, args).getRemainingArgs();<br />
&nbsp; &nbsp; if (otherArgs.length != 2) {<br />
&nbsp; &nbsp; &nbsp; System.err.println(&quot;Usage: wordcount &lt;in&gt; &lt;out&gt;&quot;);<br />
&nbsp; &nbsp; &nbsp; System.exit(2);<br />
&nbsp; &nbsp; }<br />
&nbsp; &nbsp; Job job = new Job(conf, &quot;word count&quot;);<br />
&nbsp; &nbsp; job.setJarByClass(WordCount.class);<br />
&nbsp; &nbsp; job.setMapperClass(TokenizerMapper.class);<br />
&nbsp; &nbsp; job.setCombinerClass(IntSumReducer.class);<br />
&nbsp; &nbsp; job.setReducerClass(IntSumReducer.class);<br />
&nbsp; &nbsp; job.setOutputKeyClass(Text.class);<br />
&nbsp; &nbsp; job.setOutputValueClass(IntWritable.class);<br />
&nbsp; &nbsp; FileInputFormat.addInputPath(job, new Path(otherArgs[0]));<br />
&nbsp; &nbsp; FileOutputFormat.setOutputPath(job, new Path(otherArgs[1]));<br />
&nbsp; &nbsp; System.exit(job.waitForCompletion(true) ? 0 : 1);<br />
&nbsp; }<br />
}</p>

<p>12. 编译WordCount.java<br />
[ghostfire@turing]$ mkdir wordcount<br />
[ghostfire@turing]$ cp ./WordCount.java ./wordcount<br />
[ghostfire@turing]$ cd ./wordcount<br />
[ghostfire@turing]$ mkdir classes<br />
[ghostfire@turing]$ javac -classpath /home/[你的用户名]/hadoop/hadoop-1.0.4/hadoop-core-1.0.4.jar:/home/[你的用户名]/hadoop/hadoop-1.0.4/lib/commons-cli-1.2.jar -d ./classes/ ./WordCount.java<br />
(注意,如果有同学用的是hadoop-2以上版本的,classpath和上面的区别非常大,具体请参考如下几个链接<br />
http://stackoverflow.com/questions/19223288/hadoop-2-1-0-beta-javac-compile-error<br />
http://stackoverflow.com/questions/19488894/compile-hadoop-2-2-0-job)</p>

<p><br />
[ghostfire@turing]$ jar -cvf ./WordCount.jar -C ./classes &nbsp;.<br />
(注意,打包的时候一定不能忘记了上面命令最后的点号)</p>

<p>13. 运行Hadoop作业<br />
[ghostfire@turing]$ hadoop jar ~/wordcount/WordCount.jar WordCount /user/[你的用户名]/wordcount/input &nbsp; /user/[你的用户名]/wordcount/output<br />
如果提示你说输出文件夹已经存在，那么则执行如下命令删除<br />
[ghostfire@turing]$ hadoop fs -rmr /user/[你的用户名]/wordcount/output</p>

<p>14. 获得运行结果<br />
[ghostfire@turing]$ hadoop fs -ls /user/[你的用户名]/wordcount/output<br />
[ghostfire@turing]$ hadoop fs -get /user/[你的用户名]/wordcount/output/[文件名] &nbsp;./</p>

<p>15. 关闭hadoop集群<br />
[ghostfire@turing]$ stop-all.sh&nbsp;</p>
