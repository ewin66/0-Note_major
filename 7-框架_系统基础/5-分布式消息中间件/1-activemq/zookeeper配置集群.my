<h1>zookeeper+activemq+集群消息中间件搭建</h1>

<p>原创作品，允许转载，转载时请务必以超链接形式标明文章 <a href="http://crfsz.blog.51cto.com/7835882/1872778" target="_blank">原始出处</a> 、作者信息和本声明。否则将追究法律责任。<a href="http://crfsz.blog.51cto.com/7835882/1872778">http://crfsz.blog.51cto.com/7835882/1872778</a></p>

<p><strong>ZooKeeper</strong>是一个分布式的，开放源码的分布式应用程序协调服务，它包含一个简单的原语集，分布式应用程序可以基于它实现同步服务，配置维护和命名服务等。Zookeeper是hadoop的一个子项目，在分布式应用中，由于工程师不能很好地使用锁机制，以及基于消息的协调机制不适合在某些应用中使用，因此需要有一种可靠的、可扩展的、分布式的、可配置的协调机制来统一系统的状态<br />
<br />
<strong>运行原理</strong>：Zookeeper的核心是原子广播，这个机制保证了各个Server之间的同步。实现这个机制的协议叫做Zab协议。Zab协议有两种模式，它们分别是恢复模式（选主）和广播模式（同步）。当服务启动或者在领导者崩溃后，Zab就进入了恢复模式，当领导者被选举出来，且大多数Server完成了和leader的状态同步以后，恢复模式就结束了。状态同步保证了leader和Server具有相同的系统状态。为了保证事务的顺序一致性，zookeeper采用了递增的事务id号（zxid）来标识事务。所有的提议（proposal）都在被提出的时候加上了zxid。实现中zxid是一个64位的数字，它高32位是epoch用来标识leader关系是否改变，每次一个leader被选出来，它都会有一个新的epoch，标识当前属于那个leader的统治时期。低32位用于递增计数。<br />
每个Server在工作过程中有三种状态：<br />
LOOKING：当前Server不知道leader是谁，正在搜寻<br />
LEADING：当前Server即为选举出来的leader<br />
FOLLOWING：leader已经选举出来，当前Server与之同步<br />
<br />
<br />
一、前期准备<br />
<br />
环境需要6台机器：hosts文件要保持一致<br />
<br />
编辑/etc/hosts文件：<br />
127.0.0.1   localhost localhost.localdomain localhost4localhost4.localdomain4<br />
::1         localhost localhost.localdomainlocalhost6 localhost6.localdomain6<br />
192.168.1.105 node105<br />
192.168.1.106 node106<br />
192.168.1.107 node107<br />
192.168.1.108 node108<br />
192.168.1.109 node109<br />
192.168.1.110 node110<br />
<br />
主机名定义为：node105依次排列<br />
<br />
开始安装：<br />
root身份登录系统 将jdk-7u76-linux-x64.tar.gz拷贝到/opt下面<br />
<br />
cd /opt<br />
tar zxvf jdk-7u76-linux-x64.tar.gz<br />
<br />
解压后生成的文件名字叫做jdk1.7.0_76<br />
<br />
vim /etc/profile<br />
把如下代码放到文件的最后面<br />
export JAVA_HOME=/opt/jdk1.7.0_76<br />
export JAVA_BIN=/opt/jdk1.7.0_76/bin<br />
export JRE_HOME=$JAVA_HOME/jre<br />
export CLASSPATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar:$JRE_HOME/lib/rt.jar<br />
export PATH=$PATH:$JAVA_HOME/bin:$JRE_HOME/bin<br />
export JAVA_HOME JAVA_BIN PATH CLASSPATH<br />
<br />
source/etc/profile 使变量生效<br />
<br />
<br />
二、zookeeper的部署 (三个节点上部署)<br />
192.168.1.105,192.168.1.106,192.168.1.107三个节点上要部署zookeeper<br />
将zookeeper-3.4.6.tar.gz 上传到/opt下面<br />
cd /opt<br />
tar -zxzf zookeeper-3.4.6.tar.gz<br />
ln -s zookeeper-3.4.6   zookeeper<br />
<br />
mkdir /opt/zookeeper/data     <br />
这是zk数据存储目录<br />
cd /opt/zookeeper/data<br />
vi myid<br />
如果是node105则内容为1，如果是node106则内容为2，如果是node107则内容为3</p>

<p> </p>

<p>mkdir /opt/zookeeper/logs            <br />
这是zk日志存储目录<br />
<br />
cp /opt/zookeeper/conf/zoo_sample.cfg    /opt/zookeeper/conf/zoo.cfg<br />
--zookeeper启动时会读取zookeeper/conf/zoo.cfg文件内容，zookeeper/conf/ 下面默认是没有zoo.cfg文件的, 因此我们可以根据zookeeper/conf/zoo_sample.cfg来生成zookeeper/conf/zoo.cfg<br />
vi /opt/zookeeper/conf/zoo.cfg<br />
添加如下内容<br />
dataDir=/opt/zookeeper/data<br />
dataLogDir=/opt/zookeeper/logs<br />
<br />
server.1=node105:2888:3888<br />
server.2=node106:2888:3888<br />
server.3=node107:2888:3888<br />
<br />
dataDir=/data/app/zookeeper/data<br />
dataLogDir=/data/app/zookeeper/logs<br />
<br />
<br />
各个节点启服务：<br />
/opt/zookeeper/bin/zkServer.sh start<br />
/opt/zookeeper/bin/zkServer.sh status<br />
<br />
node1<br />
/opt/zookeeper/bin/zkServer.sh status<br />
JMX enabled by default<br />
Using config: /opt/zookeeper/bin/../conf/zoo.cfg<br />
Mode: follower<br />
<br />
node2<br />
/opt/zookeeper/bin/zkServer.sh status<br />
JMX enabled by default<br />
Using config: /opt/zookeeper/bin/../conf/zoo.cfg<br />
Mode: follower<br />
<br />
node3<br />
/opt/zookeeper/bin/zkServer.sh status<br />
JMX enabled by default<br />
Using config: /opt/zookeeper/bin/../conf/zoo.cfg<br />
Mode: leader</p>

<p><br />
可见node3为zookeepr leader<br />
<br />
<br />
<br />
三、单集群activeMQ的部署<br />
规划<br />
192.168.1.105,192.168.1.106,192.168.1.107组成的是第一个集群，假设名字叫做cluster001  zookeeper+mq<br />
如果有第二个集群<br />
192.168.1.108,192.168.1.109,192.168.1.110组成的是第二个集群，假设名字叫做cluster002  mq<br />
<br />
<br />
<br />
本文档只演示cluster001的部署，cluster002的部署类似于cluster001。<br />
<br />
2.cluster001集群的部署<br />
1)分别在三台主机中创建/opt/activemq/cluster001目录 <br />
$ mkdir -p /opt/activemq/cluster001 <br />
上传apache-activemq-5.11.1-bin.tar.gz到/opt/activemq/cluster001目录 <br />
 <br />
2)解压并按节点命名 <br />
$ cd /opt/activemq/cluster001 <br />
$ tar -xvf apache-activemq-5.11.1-bin.tar.gz <br />
$ mv apache-activemq-5.11.1 node-0X   <br />
 (X代表节点号, 1表示node105、2表示node106、3表示node107，下同) <br />
  <br />
3)集群配置： <br />
 在 3 个 ActiveMQ 节点中配置 conf/activemq.xml 中的持久化适配器。修改其中 bind、zkAddress、<br />
hostname和zkPath。注意：每个ActiveMQ的BrokerName必须相同，否则不能加入集群。<br />
vim /opt/activemq/cluster001/node-01/conf/activemq.xml<br />
vim /opt/activemq/cluster001/node-02/conf/activemq.xml<br />
vim /opt/activemq/cluster001/node-03/conf/activemq.xml <br />
<br />
<br />
<br />
Node-01中的配置: <br />
<broker xmlns="http://activemq.apache.org/schema/core" brokerName="cluster001" dataDirectory="${activemq.data}"> <br />
<persistenceAdapter> <br />
<!-- kahaDB directory="${activemq.data}/kahadb"/ --> <br />
<replicatedLevelDB <br />
directory="${activemq.data}/leveldb" <br />
replicas="3" <br />
bind="tcp://0.0.0.0:62621" <br />
zkAddress="192.168.1.105:2181,192.168.1.106:2181,192.168.1.107:2181" <br />
hostname="node105" <br />
zkPath="/activemq/leveldb-stores" <br />
/> <br />
</persistenceAdapter> <br />
</broker> <br />
<br />
<destinationPolicy><br />
    <policyMap><br />
      <policyEntries><br />
        <policyEntry topic=">" ><br />
            <!-- The constantPendingMessageLimitStrategy is used to prevent<br />
                 slow topic consumers to block producers and affect other consumers<br />
                 by limiting the number of messages that are retained<br />
                 For more information, see:<br />
<br />
                 http://activemq.apache.org/slow-consumer-handling.html<br />
<br />
            --><br />
          <pendingMessageLimitStrategy><br />
            <constantPendingMessageLimitStrategy limit="1000"/><br />
          </pendingMessageLimitStrategy><br />
        </policyEntry><br />
        <br />
      <!-- 绿色标记的是要添加的代码 --><br />
<policyEntry queue=">" enableAudit="false"><br />
             <networkBridgeFilterFactory><br />
                <conditionalNetworkBridgeFilterFactory replayWhenNoConsumers="true"/> <br />
           </networkBridgeFilterFactory><br />
        </policyEntry><br />
<br />
     </policyEntries><br />
<br />
    </policyMap><br />
</destinationPolicy><br />
<br />
<br />
    Node-02中的配置: <br />
<broker xmlns="http://activemq.apache.org/schema/core" brokerName="cluster001"dataDirectory="${activemq.data}"> <br />
<persistenceAdapter> <br />
<!-- kahaDB directory="${activemq.data}/kahadb"/ --> <br />
<replicatedLevelDB <br />
directory="${activemq.data}/leveldb" <br />
replicas="3" <br />
bind="tcp://0.0.0.0:62621" <br />
zkAddress="192.168.1.105:2181,192.168.1.106:2181,192.168.1.107:2181" <br />
hostname="node106" <br />
zkPath="/activemq/leveldb-stores" <br />
      /> <br />
</persistenceAdapter> <br />
</broker> <br />
<br />
<br />
<destinationPolicy><br />
    <policyMap><br />
      <policyEntries><br />
        <policyEntry topic=">" ><br />
            <!-- The constantPendingMessageLimitStrategy is used to prevent<br />
                 slow topic consumers to block producers and affect other consumers<br />
                 by limiting the number of messages that are retained<br />
                 For more information, see:<br />
<br />
                 http://activemq.apache.org/slow-consumer-handling.html<br />
<br />
            --><br />
          <pendingMessageLimitStrategy><br />
            <constantPendingMessageLimitStrategy limit="1000"/><br />
          </pendingMessageLimitStrategy><br />
        </policyEntry><br />
        <br />
      <!-- 绿色标记的是要添加的代码 --><br />
<policyEntry queue=">" enableAudit="false"><br />
             <networkBridgeFilterFactory><br />
                <conditionalNetworkBridgeFilterFactory replayWhenNoConsumers="true"/> <br />
           </networkBridgeFilterFactory><br />
        </policyEntry><br />
<br />
     </policyEntries><br />
<br />
    </policyMap><br />
</destinationPolicy><br />
<br />
<br />
    Node-03中的配置: <br />
<broker xmlns="http://activemq.apache.org/schema/core" brokerName="cluster001" dataDirectory="${activemq.data}"> <br />
<persistenceAdapter> <br />
<!-- kahaDB directory="${activemq.data}/kahadb"/ --> <br />
<replicatedLevelDB <br />
directory="${activemq.data}/leveldb" <br />
replicas="3" <br />
bind="tcp://0.0.0.0:62621" <br />
zkAddress="192.168.1.105:2181,192.168.1.106:2181,192.168.1.107:2181" <br />
hostname="node107" <br />
zkPath="/activemq/leveldb-stores" <br />
      /> <br />
</persistenceAdapter> <br />
</broker> <br />
 <br />
<destinationPolicy><br />
    <policyMap><br />
      <policyEntries><br />
        <policyEntry topic=">" ><br />
            <!-- The constantPendingMessageLimitStrategy is used to prevent<br />
                 slow topic consumers to block producers and affect other consumers<br />
                 by limiting the number of messages that are retained<br />
                 For more information, see:<br />
<br />
                 http://activemq.apache.org/slow-consumer-handling.html<br />
<br />
            --><br />
          <pendingMessageLimitStrategy><br />
            <constantPendingMessageLimitStrategy limit="1000"/><br />
          </pendingMessageLimitStrategy><br />
        </policyEntry><br />
        <br />
      <!-- 绿色标记的是要添加的代码 --><br />
<policyEntry queue=">" enableAudit="false"><br />
             <networkBridgeFilterFactory><br />
                <conditionalNetworkBridgeFilterFactory replayWhenNoConsumers="true"/> <br />
           </networkBridgeFilterFactory><br />
        </policyEntry><br />
<br />
     </policyEntries><br />
<br />
    </policyMap><br />
</destinationPolicy><br />
<br />
4)按顺序启动3个ActiveMQ节点： <br />
$nohup /opt/activemq/cluster001/node-01/bin/activemq start &<br />
$nohup /opt/activemq/cluster001/node-02/bin/activemq start &<br />
$nohup /opt/activemq/cluster001/node-03/bin/activemq start &<br />
<br />
<br />
<br />
<br />
<br />
四、多集群activeMQ的部署<br />
这里只演示双集群的部署，三集群或n集群的部署都是类似的。<br />
<br />
修改所有主机的activemq.xml文件<br />
在<persistenceAdapter> 前面添加：<br />
<networkConnectors>  <br />
               <networkConnector  uri="multicast://default"/><br />
</networkConnectors><br />
在<transportConnector name="openwire"> 标签的末尾添加：<br />
discoveryUri="multicast://default"<br />
<br />
 </p>

<p>使用及验证：</p>

<p>#jps<br />
1522 QuorumPeerMain<br />
2661 Jps<br />
2139 activemq.jar</p>

<p>#ps -ef |grep ac 查看进程</p>

<p>找到起端口：8161 </p>

<p> </p>

<p>图示：</p>

<p>访问：<a href="http://192.168.1.150:8161/admin/" target="_blank">http://192.168.1.150:8161/admin/</a>  账户及密码： admin  admin</p>

<p> </p>

<p><a href="http://s5.51cto.com/wyfs02/M00/8A/29/wKioL1gpqaaCun_IAAFeJT7jaM8849.png" target="_blank"><img alt="wKioL1gpqaaCun_IAAFeJT7jaM8849.png" src="http://s5.51cto.com/wyfs02/M00/8A/29/wKioL1gpqaaCun_IAAFeJT7jaM8849.png" style="width:650px" title="ccc.png" /></a></p>

<p> <br />
<br />
<br />
 </p>
