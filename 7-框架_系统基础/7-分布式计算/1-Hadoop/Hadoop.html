<p>1. 安装jdk。<br />
1.1 在用户主目录下面建立jdk文件夹<br />
[ghostfire@turing]$ mkdir jdk</p>

<p>1.2 解压缩 jdk.tar.gz 文件到jdk文件夹中<br />
[ghostfire@turing]$ tar -zxvf ./jdk.tar.gz ./jdk/</p>

<p>1.3 将java路径加入path中<br />
[ghostfire@turing]$ vim ～/.bashrc<br />
在文件末尾添加如下内容并保存。<br />
    export JAVA_HOME=~/jdk/jdk1.7.0_09<br />
    export CLASSPATH=.:$JAVA_HOME/lib/*jar<br />
    export PATH=$JAVA_HOME/bin:$PATH<br />
执行如下命令，使得我们设置的path能够马上生效。<br />
[ghostfire@turing]$ source ~/.bashrc</p>

<p>1.5 检测jdk是否安装成功<br />
[ghostfire@turing]$ java -version</p>

<p><br />
2. 验证并安装ssh<br />
2.1 首先检查是否安装了ssh<br />
[ghostfire@turing]$ which ssh<br />
[ghostfire@turing]$ which sshd<br />
[ghostfire@turing]$ which ssh-keygen</p>

<p>2.2 如果提示没有安装或者无任何内容显示，执行如下命令安装ssh<br />
[ghostfire@turing]$ sudo apt-get install openssh-client<br />
[ghostfire@turing]$ sudo apt-get install openssh-server</p>

<p>2.3 检测sshd服务是否启动<br />
[ghostfire@turing]$ ps aux | grep sshd<br />
结果中若显示sshd(注意显示 grep sshd不算)，则sshd服务成功启动,否则执行如下命令启动sshd服务<br />
[ghostfire@turing]$ sudo /etc/init.d/ssh start<br />
注意在有些版本下,命令可能是 sudo /etc/init.d/sshd start</p>

<p>3.生成ssh秘钥对<br />
3.1 生成ssh公钥<br />
[ghostfire@turing]$ ssh-keygen -t rsa<br />
待输入的地方全部回车选择默认<br />
执行完毕后，会在 ~/.ssh/下面生成私钥 id_rsa,公钥id_rsa.pub</p>

<p>3.2 公钥添加<br />
[ghostfire@turing]$ cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys<br />
[ghostfire@turing]$ chmod 600 ~/.ssh/authorized_keys</p>

<p>3.3 检测公钥是否配置完成<br />
[ghostfire@turing]$ ssh localhost<br />
如果配置成功，则不需要密码就可以通过ssh进入localhost</p>

<p>4.安装hadoop<br />
4.1 在用户主目录下建立hadoop文件夹<br />
[ghostfire@turing]$ mkdir hadoop</p>

<p>4.2 解压缩hadoop-1.0.4.tar.gz<br />
[ghostfire@turing]$ tar -zxvf ./hadoop-1.0.4.tar.gz ./hadoop</p>

<p>4.3 将hadoop路径加入path<br />
[ghostfire@turing]$ vim ～/.bashrc<br />
在文件末尾添加如下内容并保存。<br />
    export HADOOP_HOME=~/hadoop/hadoop-1.0.4<br />
    export PATH=$HADOOP_HOME/bin:$PATH<br />
执行如下命令，使得我们设置的path能够马上生效。<br />
[ghostfire@turing]$ source ~/.bashrc</p>

<p>4.4 配置hadoop-env.sh<br />
修改~/hadoop/hadoop-1.0.4/conf/hadoop-env.sh<br />
在该文件最后一行添加<br />
export JAVA_HOME=~/jdk/jdk1.7.0_09</p>

<p>5. 配置单机模式<br />
对conf目录下面的配置文件不做修改即为单机模式</p>

<p>6. 配置伪分布模式<br />
6.1 修改core-site.xml文件，内容如下<br />
<?xml version="1.0"?><br />
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?><br />
<configuration><br />
    <property><br />
        <name>fs.default.name</name><br />
        <value>hdfs://localhost:9000</value><br />
    </property><br />
</configuration></p>

<p>6.2 修改mapred-site.xml文件，内容如下<br />
<?xml version="1.0"?><br />
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?><br />
<configuration><br />
    <property><br />
        <name>mapred.job.tracker</name><br />
        <value>localhost:9001</value><br />
    </property><br />
</configuration></p>

<p>6.3 修改hdfs-site.xml文件，内容如下<br />
<?xml version="1.0"?><br />
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?><br />
<configuration><br />
    <property><br />
        <name>dfs.replication</name><br />
        <value>1</value><br />
    </property><br />
<configuration></p>

<p>6.4 将localhost添加到hadoop conf目录下面的masters文件中<br />
[ghostfire@turing]$ echo "localhost" >> masters</p>

<p>6.5 将localhost添加到hadoop conf目录下面的slaves文件中<br />
[ghostfire@turing]$ echo "localhost" >> slaves</p>

<p>7. 格式化HDFS<br />
[ghostfire@turing]$ ~/hadoop/hadoop-1.0.4/bin/hadoop namenode -format</p>

<p>8. 启动hadoop<br />
[ghostfire@turing]$ ~/hadoop/hadoop-1.0.4/bin/start-all.sh</p>

<p>9. 检测hadoop是否成功启动<br />
[ghostfire@turing]$ jps<br />
TaskTracker<br />
SecondaryNameNode<br />
NameNode<br />
DateNode<br />
JobTracker</p>

<p>10. 在HDFS中添加文件和目录<br />
[ghostfire@turing]$ hadoop fs -mkdir /user/[你的用户名]/wordcount/input<br />
上面的命令本质上是递归创建的,但在有的版本上是不支持的,那么需要你依次执行如下命令<br />
[ghostfire@turing]$ hadoop fs -mkdir /user<br />
[ghostfire@turing]$ hadoop fs -mkdir /user/[你的用户名]<br />
[ghostfire@turing]$ hadoop fs -mkdir /user/[你的用户名]/wordcount<br />
[ghostfire@turing]$ hadoop fs -mkdir /user/[你的用户名]/wordcount/input</p>

<p><br />
在当前目录下面创建若干个文本文件，每个文件里面添加若干个英文单词，比如<br />
[ghostfire@turing]$ cat input1.txt<br />
no zuo no die<br />
you can you up<br />
[ghostfire@turing]$ cat input2.txt<br />
you can you up<br />
no zuo no die</p>

<p>将文本文件从本地目录上传到HDFS中<br />
[ghostfire@turing]$ hadoop fs -put ./input1.txt /user/[你的用户名]/wordcount/input<br />
[ghostfire@turing]$ hadoop fs -put ./input2.txt /user/[你的用户名]/wordcount/input</p>

<p>查看文件上传是否成功<br />
[ghostfire@turing]$ hadoop fs -lsr /</p>

<p>11. 在当前目录下新建一个WordCount.java文件<br />
import java.io.IOException;<br />
import java.util.StringTokenizer;<br />
import org.apache.hadoop.conf.Configuration;<br />
import org.apache.hadoop.fs.Path;<br />
import org.apache.hadoop.io.IntWritable;<br />
import org.apache.hadoop.io.Text;<br />
import org.apache.hadoop.mapreduce.Job;<br />
import org.apache.hadoop.mapreduce.Mapper;<br />
import org.apache.hadoop.mapreduce.Reducer;<br />
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;<br />
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;<br />
import org.apache.hadoop.util.GenericOptionsParser;</p>

<p>public class WordCount {</p>

<p>  public static class TokenizerMapper <br />
       extends Mapper<Object, Text, Text, IntWritable>{<br />
    <br />
    private final static IntWritable one = new IntWritable(1);<br />
    private Text word = new Text();<br />
      <br />
    public void map(Object key, Text value, Context context<br />
                    ) throws IOException, InterruptedException {<br />
      StringTokenizer itr = new StringTokenizer(value.toString());<br />
      while (itr.hasMoreTokens()) {<br />
        word.set(itr.nextToken());<br />
        context.write(word, one);<br />
      }<br />
    }<br />
  }<br />
  <br />
  public static class IntSumReducer <br />
       extends Reducer<Text,IntWritable,Text,IntWritable> {<br />
    private IntWritable result = new IntWritable();</p>

<p>    public void reduce(Text key, Iterable<IntWritable> values, <br />
                       Context context<br />
                       ) throws IOException, InterruptedException {<br />
      int sum = 0;<br />
      for (IntWritable val : values) {<br />
        sum += val.get();<br />
      }<br />
      result.set(sum);<br />
      context.write(key, result);<br />
    }<br />
  }</p>

<p>  public static void main(String[] args) throws Exception {<br />
    Configuration conf = new Configuration();<br />
    String[] otherArgs = new GenericOptionsParser(conf, args).getRemainingArgs();<br />
    if (otherArgs.length != 2) {<br />
      System.err.println("Usage: wordcount <in> <out>");<br />
      System.exit(2);<br />
    }<br />
    Job job = new Job(conf, "word count");<br />
    job.setJarByClass(WordCount.class);<br />
    job.setMapperClass(TokenizerMapper.class);<br />
    job.setCombinerClass(IntSumReducer.class);<br />
    job.setReducerClass(IntSumReducer.class);<br />
    job.setOutputKeyClass(Text.class);<br />
    job.setOutputValueClass(IntWritable.class);<br />
    FileInputFormat.addInputPath(job, new Path(otherArgs[0]));<br />
    FileOutputFormat.setOutputPath(job, new Path(otherArgs[1]));<br />
    System.exit(job.waitForCompletion(true) ? 0 : 1);<br />
  }<br />
}</p>

<p>12. 编译WordCount.java<br />
[ghostfire@turing]$ mkdir wordcount<br />
[ghostfire@turing]$ cp ./WordCount.java ./wordcount<br />
[ghostfire@turing]$ cd ./wordcount<br />
[ghostfire@turing]$ mkdir classes<br />
[ghostfire@turing]$ javac -classpath /home/[你的用户名]/hadoop/hadoop-1.0.4/hadoop-core-1.0.4.jar:/home/[你的用户名]/hadoop/hadoop-1.0.4/lib/commons-cli-1.2.jar -d ./classes/ ./WordCount.java<br />
(注意,如果有同学用的是hadoop-2以上版本的,classpath和上面的区别非常大,具体请参考如下几个链接<br />
http://stackoverflow.com/questions/19223288/hadoop-2-1-0-beta-javac-compile-error<br />
http://stackoverflow.com/questions/19488894/compile-hadoop-2-2-0-job)</p>

<p><br />
[ghostfire@turing]$ jar -cvf ./WordCount.jar -C ./classes  .<br />
(注意,打包的时候一定不能忘记了上面命令最后的点号)</p>

<p>13. 运行Hadoop作业<br />
[ghostfire@turing]$ hadoop jar ~/wordcount/WordCount.jar WordCount /user/[你的用户名]/wordcount/input   /user/[你的用户名]/wordcount/output<br />
如果提示你说输出文件夹已经存在，那么则执行如下命令删除<br />
[ghostfire@turing]$ hadoop fs -rmr /user/[你的用户名]/wordcount/output</p>

<p>14. 获得运行结果<br />
[ghostfire@turing]$ hadoop fs -ls /user/[你的用户名]/wordcount/output<br />
[ghostfire@turing]$ hadoop fs -get /user/[你的用户名]/wordcount/output/[文件名]  ./</p>

<p>15. 关闭hadoop集群<br />
[ghostfire@turing]$ stop-all.sh </p>
